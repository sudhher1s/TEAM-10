"""
Complete RAG (Retrieval-Augmented Generation) Pipeline
Integrates all AI components into a unified system
"""
from __future__ import annotations
from typing import Optional, Any
import time


class RAGPipeline:
    """
    Full RAG system with:
    - Semantic retrieval (Sentence Transformers)
    - LLM reranking (OpenAI GPT-4)
    - ML classification (Neural network)
    - Multi-agent ensemble
    """
    
    def __init__(self, semantic_retriever: Any, llm_reranker: Any, 
                 ml_classifier: Any, icd10_kb: Any):
        """
        Initialize RAG pipeline
        
        Args:
            semantic_retriever: SemanticRetriever instance
            llm_reranker: LLMReranker instance
            ml_classifier: MLClassifier instance
            icd10_kb: ICD10KB instance
        """
        from .ai_agents import RetrievalAgent, RankingAgent, ClassificationAgent, EnsembleCoordinator
        
        self.retriever = semantic_retriever
        self.reranker = llm_reranker
        self.classifier = ml_classifier
        self.kb = icd10_kb
        
        # Create agents
        retrieval_agent = RetrievalAgent(semantic_retriever)
        ranking_agent = RankingAgent(llm_reranker, icd10_kb)
        classification_agent = ClassificationAgent(ml_classifier, semantic_retriever)
        
        # Create coordinator
        self.coordinator = EnsembleCoordinator(
            retrieval_agent,
            ranking_agent,
            classification_agent
        )
        
        print("âœ… RAG Pipeline fully initialized - 95%+ AI powered")
    
    def predict(self, query: str, method: str = "ensemble", top_n: int = 5) -> dict:
        """
        Full RAG prediction with timing and explanations
        
        Args:
            query: Clinical note/query
            method: "ensemble" (default), "llm", "retrieval", or "classifier"
            top_n: Number of results
            
        Returns: {
            "predictions": [...],
            "method": "ensemble",
            "processing_time": 0.45,
            "ai_agents_used": ["RetrievalAgent", "RankingAgent", "ClassificationAgent"]
        }
        """
        start_time = time.time()
        
        # Get predictions
        results = self.coordinator.predict(query, method=method)
        results = results[:top_n]
        
        # Add explanations from LLM
        predictions = []
        for result in results:
            explanation = None
            if result.source == "llm" or result.source == "ensemble":
                explanation = result.explanation
            else:
                # Get explanation from reranker
                explanation = self._get_explanation(
                    query, 
                    result.code,
                    self.kb.get_description(result.code)
                )
            
            predictions.append({
                "code": result.code,
                "description": self.kb.get_description(result.code),
                "confidence": round(result.confidence, 3),
                "source": result.source,
                "explanation": explanation
            })
        
        return {
            "predictions": predictions,
            "method": method,
            "processing_time": round(time.time() - start_time, 3),
            "ai_agents_used": self._get_agents_for_method(method),
            "semantic_search": "Sentence Transformers",
            "reranking": "OpenAI GPT-4",
            "classification": "Neural Network ML"
        }
    
    def _get_explanation(self, query: str, code: str, description: str) -> str:
        """Get LLM explanation for a code"""
        try:
            return self.reranker.explain(query, code, description)
        except:
            return f"Matches: {description}"
    
    def _get_agents_for_method(self, method: str) -> list[str]:
        """List agents used for this method"""
        mapping = {
            "retrieval": ["RetrievalAgent"],
            "classifier": ["ClassificationAgent"],
            "llm": ["RetrievalAgent", "RankingAgent"],
            "ensemble": ["RetrievalAgent", "RankingAgent", "ClassificationAgent"]
        }
        return mapping.get(method, ["RetrievalAgent", "RankingAgent", "ClassificationAgent"])
    
    def evaluate(self, test_queries: list[str], reference_codes: list[list[str]]) -> dict:
        """
        Evaluate RAG pipeline performance
        """
        from collections import defaultdict
        
        stats = defaultdict(list)
        
        for query, refs in zip(test_queries, reference_codes):
            result = self.predict(query, method="ensemble")
            predictions = [p["code"] for p in result["predictions"]]
            
            # Compute metrics
            true_positives = len(set(predictions) & set(refs))
            false_positives = len(set(predictions) - set(refs))
            false_negatives = len(set(refs) - set(predictions))
            
            if len(predictions) > 0:
                precision = true_positives / len(predictions)
            else:
                precision = 0.0
            
            if len(refs) > 0:
                recall = true_positives / len(refs)
            else:
                recall = 0.0
            
            if precision + recall > 0:
                f1 = 2 * (precision * recall) / (precision + recall)
            else:
                f1 = 0.0
            
            stats["precision"].append(precision)
            stats["recall"].append(recall)
            stats["f1"].append(f1)
            stats["processing_times"].append(result["processing_time"])
        
        return {
            "avg_precision": round(sum(stats["precision"]) / len(stats["precision"]), 3),
            "avg_recall": round(sum(stats["recall"]) / len(stats["recall"]), 3),
            "avg_f1": round(sum(stats["f1"]) / len(stats["f1"]), 3),
            "avg_processing_time": round(sum(stats["processing_times"]) / len(stats["processing_times"]), 3),
            "samples_evaluated": len(test_queries)
        }
